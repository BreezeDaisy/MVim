{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06798712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121, 89, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# 读取图像\n",
    "image = cv2.imread(\"/home/zdx/python_daima/MVim/MVim/Face_Pose/storage/driver_face.jpg\")\n",
    "\n",
    "\n",
    "# 输出图像size\n",
    "print(image.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb8f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# 读取图像\n",
    "image = cv2.imread(\"/home/zdx/python_daima/MVim/MVim/data/SFDDD/images/test/img_1.jpg\")\n",
    "\n",
    "\n",
    "# 输出图像size\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79196a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "# 读取图像\n",
    "image = cv2.imread(\"/home/zdx/python_daima/MVim/MVim/Face_Pose/storage/img_1.jpg\")\n",
    "\n",
    "# 将图像的size重塑为(224, 224)\n",
    "image = cv2.resize(image, (480, 480))\n",
    "\n",
    "# 保存图像\n",
    "cv2.imwrite(\"/home/zdx/python_daima/MVim/MVim/Face_Pose/storage/img_1_resized.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f7f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 14:36:37.853839: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-27 14:36:37.897412: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-27 14:36:39.159796: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/zdx/anaconda3/envs/manager/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1. 打印torch版本和CUDA支持\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch版本：\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[0;32m~/anaconda3/envs/manager/lib/python3.10/site-packages/libreface/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetect_mediapipe_image\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAU_Detection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m detect_action_units, detect_action_units_video\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAU_Recognition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_au_intensities, get_au_intensities_and_detect_aus, get_au_intensities_and_detect_aus_video, get_au_intensities_video\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mFacial_Expression_Recognition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_facial_expression, get_facial_expression_video\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_frames_from_video_ffmpeg, uniquify_file, check_file_type\n",
      "File \u001b[0;32m~/anaconda3/envs/manager/lib/python3.10/site-packages/libreface/AU_Recognition/inference.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAU_Recognition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolver_inference_image\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m solver_inference_image\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAU_Recognition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolver_inference_combine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m solver_inference_image_task_combine\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mConfigObject\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/manager/lib/python3.10/site-packages/libreface/AU_Recognition/solver_inference_image.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAU_Recognition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresnet18\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResNet18\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAU_Recognition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MaskedAutoEncoder\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_weights\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimage_test\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/manager/lib/python3.10/site-packages/libreface/AU_Recognition/models/mae.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# import models.modeling_pretrain\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibreface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAU_Recognition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmasking_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomMaskingGenerator\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMaskedAutoEncoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import libreface as lf\n",
    "\n",
    "# 1. 打印torch版本和CUDA支持\n",
    "print(\"Torch版本：\", torch.__version__)\n",
    "print(\"CUDA可用：\", torch.cuda.is_available())\n",
    "\n",
    "# 加载模型时忽略不匹配的键\n",
    "model = lf.load_model(\"emotion+au\")\n",
    "state_dict = torch.load(model.weight_path, map_location=\"cuda\")  # 用GPU加载\n",
    "model.load_state_dict(state_dict, strict=False)  # strict=False 忽略无关键\n",
    "\n",
    "# 2. 加载LibreFace模型（GPU模式）\n",
    "model = lf.load_model(\"emotion\")  # 加载基础情绪分类模型\n",
    "model = model.to(\"cuda\")  # 移到GPU\n",
    "\n",
    "# 3. 测试单张图片推理（替换为你的图片路径）\n",
    "img_path = \"/home/zdx/python_daima/MVim/MVim/Face_Pose/Emotion_dataset/Neutral/image_63.png\"\n",
    "pred = model.infer(img_path)\n",
    "\n",
    "# 4. 打印结果（能输出则说明完全兼容）\n",
    "print(\"情绪预测结果：\", pred[\"emotion\"])\n",
    "print(\"置信度：\", pred[\"confidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecab8052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ torch版本： 2.2.2+cu118\n",
      "✅ CUDA可用： True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 2. 验证torch+libreface依赖（关联之前的问题）\n",
    "print(\"✅ torch版本：\", torch.__version__)\n",
    "print(\"✅ CUDA可用：\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84f02b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "# 读取图像\n",
    "image = cv2.imread(\"/home/zdx/python_daima/MVim/MVim/Face_Pose/Constructed_Emotion_Dataset/train/antipathic/train_0.jpg\")\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "# 将图像的size重塑为(112, 112),不会剪裁图像\n",
    "image = cv2.resize(image, (112, 112))\n",
    "\n",
    "# 保存图像\n",
    "cv2.imwrite(\"/home/zdx/python_daima/MVim/MVim/Face_Pose/storage/constructed_dataset_resize.jpg\", image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
